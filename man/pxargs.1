.TH pxargs 1 "December 31,  2014
.SH NAME
pxargs \- build and execute command lines from a newline delimited list in FILE or from standard input, run in parallel using mpi
.SH USAGE:
.B pxargs 
[
.B OPTIONS
] 
-p <exec>

.SH DESCRIPTION
The pxargs utility reads newline delimited strings from the standard input or from a file and executes exec with the strings as arguments.
Blank lines on the standard input or in the file are ignored. The command may be executed across hosts in parallel using mpi, typically 
on some basic "compute cluster" of nodes with openmpi, lam, etc... installed. pxargs is loosely modeled after the venerable xargs(1) program. 

.TP
.B --arg-file=<file>, -a <file>
The path to the arg list
.br
The file format is:
.nf
<args_1>
<  .   > 
<  .   > 
<args_n> 
.fi

.TP
.B --proc=<exec>, -p <exec>
The executable or script for processing each item given. This script will get called for each
line in --arg-file, e.g.
.br
.nf
exec <args_1>
 .   <  .   > 
 .   <  .   > 
exec <args_n> 
.fi
.br

.TP
.B --work-analyze, -w
This option prints out item execution time statistics and information when all work is completed.

.TP
.B --random-starts=<n-m>, -r <n-m>
Randomize the initial starts on the interval n to m, where n and m are seconds and n is less than or equal to m. During 
the initial distribution phase, the work units/items will have a random (uniform) staggered start to desynchronize item execution. 
The initial distribution will release work items up to numprocs while waiting between each release, then transition into a
producer-consumer mode with no waiting between work items release.
.br
Pseudo code to describe the above:
.br
for number of processors-1:
.br
   wait random seconds
.br
   send work to processor i
.br
done for
.br
now transition to a producer consumer mode until done with all items.

.p
This functionality may be useful to avoid all units/items accessing a file(s) at the same time, i.e. desynchronize their execution states. 

.TP
.B --max-time, -m <n>
The maximum run-time in seconds allowed for an arg/work unit. If an "exec args_i" runs longer 
than n seconds the worker process will kill that particular unit and any child processes, and 
then let the next one begin processing (if another unit is available). This may help with a 'runaway' 
process. Note that a worker sub process kill on the n seconds may not be exact, pending system "busyness" 
for the alarm signal delivery, so pad the 'n' by a few seconds if needed.

.TP
.B -n, --not-complete <path> 
A file that pxargs will attempt to write out unfinished work at pre-exit. This option requires that pxargs is compiled against libpbs. 
When this option is used pxargs will use ONE mpi proc to monitor and react to pre-exit "events" (or other events). When using pbs, you 
can pack ONE node with more processors than is available, as this ONE mpi process is not really doing much work. See the pbs qsub options 
on how to "crowd" one node with more mpi procs (chunks). The rank ordering/tasking: rank 0 is the queue (master), rank 1 is the monitor 
(when this option is used), all others ranks (consumers) do work.

Note, when the option -r | --random-starts is used with this option the processing must have transitioned to the producer 
consumer mode before a "pre-exit" event occurs or else a successful dump of the incomplete may not complete; in other words,
choose a reasonable wall time with submitting the job(s) to pbs.

.TP
.B -t <sec> 
This arg suggests the time, in seconds, before the job is due to get the exit signal from pbs to write out the "not-complete" 
or check point file (see -n option). This option is only used if the -n <file> option is set. Note <sec> is only a suggestion and not
exact. The default is 60 seconds (#define DEFAULT_PREEXIT_SECONDS)

.TP
.B --help, -h
Display a terse help message then exit.

.TP
.B  --verbose, -v
Run the program in verbose mode. Add more -v for more verbosity. 

.TP
.B  --version, -V
Print the version number of pxargs and exit.


.SH EXAMPLES
.br

mpiexec -np 16 pxargs --arg-file=arglist.txt --proc=/bin/tileproc 
.br
The above command will start 16 processes and run /bin/tileproc on all items in arglist.txt, arglist.txt has a list 
of arguments separated by newlines.
.br

mpiexec -np 16 pxargs --proc=/bin/tileproc < arglist.txt
.br
The above command will start 16 processes and run /bin/tileproc on all items given in arglist.txt
.br

mpiexec -np 16 pxargs --arg-file=arglist.txt --proc=/bin/tileproc -v -v -v --random-starts 1-6 --work-analyze 
.br
The above command will start 16 processes, randomly wait between 1 and 6 seconds while sending the 
initial batch of work items, then process the rest of the work items until there is no more work, then 
print out each unit's/item's process time. Note, the waiting is only done on the initial work/item
distribution phase, see the --random-starts option.
.br

mpiexec -np 8192 pxargs --arg-file=arglist.txt --proc=/bin/tileproc -v -v -m 180 -w --not-complete=newArgList.txt -t 240
.br
The above command will start 8192 processes and begin chewing away at the work listed in arglist.txt using the 
serial program tileproc. Any tileproc process that takes longer than 180 seconds to execute will be killed, since
we expect the tileproc execution time per unit to be completed within 120 seconds at most (-m). The maximum wall time we 
could request for the entire job (via pbs qsub) is 8 hours, but we know that in order for all items in arglist.txt 
to complete we need ~96 hours so 240 seconds (4 minutes) before the 8 hour job is killed by pbs pxargs will 
asynchronously take inventory of which items in arglist.txt have not completed and write these out to 
newArgList.txt, which can be used to resubmit another job, repeat until done... 
.br


.SH ENVIRONMENT
Standard mpi environment applies. When using a monitor process (-n option), the node on which the process (rank 1) is running must 
be able to contact the pbs scheduler, i.e. it cannot be blocked by firewalls, lack of network routes, etc... 

.SH ASSUMPTIONS
All data files or input files given via the arglist are accessible via a global file system like gfs, nfs, cfs, luster, etc.. 
This utility only works well for command lines that take longer than a ~1/4 second to execute due to comm overhead.

.SH NOTES
Why is the monitoring facility placed in it's own mpi proc, isn't that wasteful as it doesn't do much? The reason is that
communications with external services, signal handling and threading activity can cause issues on some platforms and mpi 
implementations if the monitor is threaded within the coordinator/master process (rank 0). 
See pbs resource "chunking".

.SH SEE ALSO
xargs(1)

.SH AUTHOR
Andrew Michaelis 

.SH BUGS
Please email bugs to: amac at hyperplane dot org 

